{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2\n",
    "\n",
    "# after you pull run `ollama run llama3.2`\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Generation**: AI-powered tools can create high-quality content such as articles, social media posts, product descriptions, and more, saving time and resources for businesses.\n",
      "2. **Image and Video Creation**: Generative AI can produce realistic images, videos, and 3D models for applications like advertising, entertainment, and product visualization.\n",
      "3. **Chatbots and Virtual Assistants**: AI-powered chatbots can provide customer support, answer frequently asked questions, and offer personalized experiences, improving user engagement and satisfaction.\n",
      "4. **Predictive Analytics and Forecasting**: Generative AI algorithms can analyze large datasets to predict future trends, identify patterns, and make predictions on sales, demand, and more.\n",
      "5. **Personalization**: By generating content tailored to individual preferences, businesses can create a more personalized experience for customers, increasing loyalty and engagement.\n",
      "6. **Design and Prototyping**: Generative AI can assist in the design process by generating multiple designs, prototypes, and mockups, saving time and resources for designers and product developers.\n",
      "7. **Marketing Automation**: AI-powered tools can automate repetitive marketing tasks, such as email campaigns, lead generation, and social media posting, allowing businesses to focus on high-value activities.\n",
      "8. **Sales and Lead Generation**: Generative AI can help generate new leads by analyzing customer data, identifying potential buyers, and creating personalized sales pitches.\n",
      "9. **Data Analysis and Visualization**: AI-powered tools can analyze large datasets, identify patterns, and create interactive visualizations, helping businesses gain insights and make data-driven decisions.\n",
      "10. **Product Development**: Generative AI can assist in the development process by generating ideas, prototypes, and designs for new products, reducing time and costs.\n",
      "\n",
      "Some specific business applications of Generative AI include:\n",
      "\n",
      "* **Fashion and Apparel**: Designing new product lines, generating virtual try-on experiences, and creating personalized fashion recommendations.\n",
      "* **Finance and Banking**: Creating automated investment tools, generating financial forecasts, and providing personalized customer service.\n",
      "* **Healthcare**: Developing personalized treatment plans, generating patient profiles, and analyzing medical images for diagnosis.\n",
      "* **E-commerce**: Generating product descriptions, optimizing product listings, and creating personalized product recommendations.\n",
      "* **Education**: Creating personalized learning experiences, generating educational content, and assessing student performance.\n",
      "\n",
      "These are just a few examples of the many business applications of Generative AI. As the technology continues to evolve, we can expect to see even more innovative uses across various industries.\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI has numerous business applications across various industries, including:\n",
      "\n",
      "1. **Content Creation**: Generative AI can generate high-quality content such as text, images, videos, and music that is indistinguishable from human-created content. This technology can be used for social media management, blog posts, product descriptions, and more.\n",
      "2. **Marketing Automation**: Generative AI can help automate marketing tasks such as email templates, ad copywriting, and lead generation. It can also analyze customer behavior and generate personalized content recommendations.\n",
      "3. **Product Design**: Generative AI can design new products using complex algorithms that simulate human-like creativity. This can be used for product designers to brainstorm ideas or create product prototypes.\n",
      "4. **Customer Service Chatbots**: Generative AI can power chatbots that respond to customer inquiries with empathetic and personalized responses, improving the overall customer experience.\n",
      "5. **Predictive Maintenance**: Generative AI can analyze sensor data from industrial equipment and predict when maintenance is required, reducing downtime and increasing efficiency.\n",
      "6. **Autonomous Vehicles**: Generative AI can be used to generate 3D models of virtual driving environments, which helps developers to enhance their self-driving vehicle simulations and testing processes.\n",
      "7. **Human Resources**: Generative AI can help in employee profiling, candidate matching, and interview prep tools.\n",
      "8. **Financial Analysis**: Generative AI can analyze large datasets to identify patterns, trends, and predictions that were previously unknown.\n",
      "9. **Supply Chain Optimization**: Generative AI can analyze supply chain data to optimize inventory management, transportation routes, and other logistical parameters for improved efficiency and reduced costs.\n",
      "\n",
      "The benefits of using Generative AI in business applications include:\n",
      "\n",
      "1. Increased speed and agility\n",
      "2. Improved accuracy and precision\n",
      "3. Enhanced creativity and innovation\n",
      "4. Reduced costs and increased productivity\n",
      "5. Real-time decision-making\n",
      "\n",
      "However, there are also potential risks associated with Generative AI applications, such as:\n",
      "\n",
      "1. **Bias and Discrimination**: Biased models can perpetuate existing social inequalities or biases.\n",
      "2. **Job Replacement**: Automation posed by Generative AI could displace certain jobs or tasks.\n",
      "3. **Intellectual Property Issues**: Looming questions arise regarding the ownership of content generated using Generative AI tools.\n",
      "4. **Cybersecurity**: Weaknesses in AI code may leave systems vulnerable to hacking and exploitation.\n",
      "\n",
      "These risks and challenges must be carefully considered as Generative AI becomes more integrated into business processes and decision-making mechanisms.\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling aabd4debf0c8... 100% ▕████████████████▏ 1.1 GB                         \n",
      "pulling 369ca498f347... 100% ▕████████████████▏  387 B                         \n",
      "pulling 6e4c38e1172f... 100% ▕████████████████▏ 1.1 KB                         \n",
      "pulling f4d24e9138dd... 100% ▕████████████████▏  148 B                         \n",
      "pulling a85fe2a2e58e... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "<think>\n",
      "Okay, so I'm trying to understand the basics of how large language models work. I know that they're generated by training a neural network on vast amounts of text data, but I'm not exactly sure what those underpinnings are all about. The user asked for definitions related to neural networks, attention modules, and the transformer architecture.\n",
      "\n",
      "Starting with the neural network core concepts: from what I remember, these models use layers where each layer contains a set of neurons. Each neuron processes some input information, applies an activation function, and passes the output on to the next layer. They learn by processing repeated forward and backward pass through the network—forward being data flowing into the model, forward-propagation is processing in a forward direction, then backward or backpropagation is using errors from a loss function to update the weights.\n",
      "\n",
      "But I'm not entirely clear on how these layers work together. It's like each layer processes the output of one before moving on, right? So, in a neural network, multiple layers combine and transform the data. Each layer can learn different kinds of features because it applies non-linear transformations. This allows the network to model complex patterns.\n",
      "\n",
      "Moving on to attention: that's about focusing on what's most relevant when processing information. I think it's where models don't just look at all parts of the input, but instead attend to specific positions or tokens in the sequence. So when generating text, an ML model chooses which words are important for a sentence structure rather than using every word.\n",
      "\n",
      "Wait, but how is attention implemented? Is it through something like scaled dot-product attention where each token computes a similarity score and picks the most similar one with some probability? That must be a way to weigh which parts of the input are relevant.\n",
      "\n",
      "Now, the Transformer architecture— this part I'm more familiar with. From what I've read before, transformers process sequences in parallel because their multi-head attention layers allow all tokens to attend to each other simultaneously. Each token is passed through multiple attention mechanisms where it computes a focused context vector by paying attention to different groups of words.\n",
      "\n",
      "This leads to an idea called \"attention masking,\" which ignores irrelevant parts of the input and focuses on more relevant ones. So for generating text, the model can choose certain tokens when they're most informative for the conversation flow.\n",
      "\n",
      "Hmm, how does this lead to better performance? I think it's because each model is only focused on what's needed. When the user sends a sentence, the Transformer knows that only parts of it are relevant, so it doesn't get bogged down by extraneous words or punctuation. That should help with tasks like answering questions precisely.\n",
      "\n",
      "Putting this together: Each Transformer layer has multiple attention heads, each checking all other tokens to see what's most important for their output. The result is a way for each part of the text model to have its specific focus within larger sequences. This allows for more precise processing because there's less redundancy of word features being learned.\n",
      "\n",
      "I think I'm getting the basic idea, but maybe I need to visualize it or get some examples. Like, imagine a sentence generator trying to translate sentences into another language— each token in the input is processed by attention focusing on which ones are crucial for translating each part correctly. So even if there's noise in the words near the important parts, the model can ignore them and focus on what matters most.\n",
      "\n",
      "Wait, but how do these attention modules connect across layers? I thought that once a sentence processes a certain window of tokens through an attention mechanism, it then feeds back to the same layer with different windows for each function or produces a final output. That way, the model can dynamically adjust its focus during different stages of processing.\n",
      "\n",
      "Are there any limitations or challenges related to this architecture? Well, I know that the size of the model (number of layers and attention heads) affects performance. Too small and it's just pattern memorization; too big might overfit. Also, computational resources are a big consideration because models can be huge in parameters.\n",
      "\n",
      "Overall, it all comes down to how efficient each model is at focusing on relevant information using attention, which makes them so good for precise tasks where context matter a lot.\n",
      "</think>\n",
      "\n",
      "**Core Concepts Behind Large Language Models (LLMs): A Deep dive into Neural Networks, Attention, and Transformers**\n",
      "\n",
      "**Neural Network Core Concepts:**\n",
      "1. **Layered Architecture:** Neurons within neural networks process input information by applying non-linear transformations. These layers work by combining, transforming, and shipping data forward or backward through the network.\n",
      "2. **Feature Computation:** Each neuron processes an input, applies an activation function (e.g., ReLU), and passes the result for further processing in subsequent layers. This allows the model to gradually learn more complex features from raw data.\n",
      "3. **Learning Mechanism:** The network's ability to learn through repeated forward and backward propagation enables it to adjust weights based on error gradients, improving its performance over training.\n",
      "\n",
      "**Attention Module Concept:**\n",
      "1. **Focus on Relevant Information:** Attention mechanisms allow models to focus on specific elements of input data that are most relevant for the task at hand.\n",
      "2. **Weighted Summing:** Models compute a weighted sum of all words, selecting those with higher relevance scores. This reduces redundancy by ignoring irrelevant information.\n",
      "3. **Probabilistic Selection:** Each token receives attention scores from other tokens, which determine how likely it is to influence the model's output.\n",
      "\n",
      "**Transformer Architecture:**\n",
      "1. **Parallel Processing:** The Transformer processes input sequences in parallel across all tokens due to its multi-head attention mechanism. This allows simultaneous computation of context vectors for each part of the sequence.\n",
      "2. **Multi-Head Attention Mechanism:** Each token attends to all others in a set window, enabling efficient interaction between different parts of the input at the same time step.\n",
      "3. **Focus Masking:** By ignoring irrelevant sections and focusing on relevant parts through attention masks, models are more precise in their processing.\n",
      "\n",
      "**How These Concepts Lead to Performance:**\n",
      "1. **Locality Focus:** Transformers focus processing on local information within each token's context window, reducing redundancy and emphasizing specific sentence structures.\n",
      "2. **Efficient Processing:** By dynamically adjusting which context vectors are used at different stages of generating text or answering questions, models can ensure specificity without excessive noise.\n",
      "\n",
      "**Challenges and Considerations:**\n",
      "1. **Model Size (Complexity):** Larger models require more parameters, affecting both memorization capacity and efficiency.\n",
      "2. **Computational Requirements:** The significant computational resources needed for training large models pose a challenge in accessibility for everyday use.\n",
      "\n",
      "In summary, the Transformer architecture's focus on relevant information through attention module enables precise processing of text data, making it highly effective for tasks requiring deep context understanding. While with its increased complexity and potential downsides, this model type is revolutionizing natural language generation by enhancing both efficiency and specificity.\n"
     ]
    }
   ],
   "source": [
    "!ollama pull deepseek-r1:1.5b\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Please give definitions of some core concepts behind LLMs: a neural network, attention and the transformer\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit - Dive into anything\n"
     ]
    }
   ],
   "source": [
    "result = Website(\"https://www.reddit.com/\")\n",
    "print(result.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(website): \n",
    "    prompt = user_prompt_for(website)\n",
    "    if not prompt:\n",
    "        raise ValueError(\"user_prompt_for(website) returned an empty string!\")\n",
    "    \n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the Ollama function instead of OpenAI\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    messages = messages_for(website)\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of the Website Reddit**\n",
       "\n",
       "* **News and Announcements**\n",
       "\t+ Kendrick Lamar's streams for \"Not Like Us\" have soared 430% on Spotify after his Super Bowl halftime show performance.\n",
       "\t+ The CEO of OpenAI has rejected a $97.4 billion bid from Elon Musk, saying 'no thank you'.\n",
       "\t+ A plane crash at the Scottsdale airport has closed the runway and left injuries unclear.\n",
       "* **Popular Communities**\n",
       "\t+ r/AskReddit (51.4 million members) is one of the most popular communities on Reddit.\n",
       "\t+ r/gtaonline (1.6 million members) and r/Twitch (2.8 million members) are also among the top communities.\n",
       "* **Subreddits**\n",
       "\t+ The website has thousands of subreddits covering various topics, including music, news, gaming, technology, pop culture, and more.\n",
       "\n",
       "Overall, Reddit is a vast platform with a wide range of content and communities centered around different interests."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "\n",
    "display_summary(\"https://reddit.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
